#!/usr/bin/env python3
"""
Performance Analysis for Oracle-X Data Feed System

Analyzes timing results from comprehensive testing to identify optimization opportunities.
"""

import json
import os
from datetime import datetime
from typing import Dict, List, Any


def analyze_performance():
    """Analyze performance data from comprehensive testing"""

    # Load test results
    results_file = "test_results_comprehensive.json"
    if not os.path.exists(results_file):
        print(f"‚ùå Test results file not found: {results_file}")
        print(
            "Run the comprehensive tests first: python test_data_feeds_comprehensive.py"
        )
        return

    with open(results_file, "r") as f:
        test_results = json.load(f)

    print("=== Oracle-X Performance Analysis ===")
    print(f"Analysis Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()

    # Categorize tests
    adapter_tests = []
    orchestrator_tests = []
    cli_tests = []

    for result in test_results:
        if result.get("timing_ms") is None:
            continue  # Skip skipped tests

        test_name = result["test"]
        timing = result["timing_ms"]

        if "orchestrator" in test_name:
            orchestrator_tests.append((test_name, timing))
        elif "cli" in test_name:
            cli_tests.append((test_name, timing))
        else:
            adapter_tests.append((test_name, timing))

    # Individual Adapter Performance
    print("## Individual Adapter Performance")
    print("| Adapter | Time (ms) | Performance |")
    print("|---------|-----------|-------------|")

    adapter_tests.sort(key=lambda x: x[1])  # Sort by timing

    for test_name, timing in adapter_tests:
        if timing < 500:
            performance = "üöÄ Excellent"
        elif timing < 1000:
            performance = "‚úÖ Good"
        elif timing < 2000:
            performance = "‚ö†Ô∏è Acceptable"
        else:
            performance = "üêå Slow"

        print(f"| {test_name} | {timing:.0f} | {performance} |")

    print()

    # Orchestrator Performance
    print("## Orchestrator Performance")
    print("| Operation | Time (ms) | Performance |")
    print("|-----------|-----------|-------------|")

    orchestrator_tests.sort(key=lambda x: x[1])

    for test_name, timing in orchestrator_tests:
        operation = test_name.replace("orchestrator_", "")

        if operation == "init":
            if timing < 100:
                performance = "üöÄ Excellent"
            else:
                performance = "‚ö†Ô∏è Check initialization"
        elif operation == "quote":
            if timing < 1000:
                performance = "üöÄ Excellent"
            elif timing < 2000:
                performance = "‚úÖ Good"
            else:
                performance = "‚ö†Ô∏è Review sources"
        elif operation == "sentiment":
            if timing < 5000:
                performance = "‚úÖ Good"
            elif timing < 10000:
                performance = "‚ö†Ô∏è Acceptable"
            else:
                performance = "üêå Needs optimization"
        elif operation == "market_data":
            if timing < 1000:
                performance = "üöÄ Excellent"
            elif timing < 2000:
                performance = "‚úÖ Good"
            else:
                performance = "‚ö†Ô∏è Review sources"
        else:
            performance = "üìä Analysis needed"

        print(f"| {operation} | {timing:.0f} | {performance} |")

    print()

    # CLI Performance
    print("## CLI Integration Performance")
    print("| CLI Command | Time (ms) | Performance |")
    print("|-------------|-----------|-------------|")

    cli_tests.sort(key=lambda x: x[1])

    for test_name, timing in cli_tests:
        command = test_name.replace("cli_validate_", "")

        if timing < 2000:
            performance = "üöÄ Excellent"
        elif timing < 4000:
            performance = "‚úÖ Good"
        else:
            performance = "‚ö†Ô∏è Review overhead"

        print(f"| {command} | {timing:.0f} | {performance} |")

    print()

    # Performance Recommendations
    print("## Performance Optimization Recommendations")
    print()

    # Find slowest adapters
    slow_adapters = [test for test in adapter_tests if test[1] > 2000]
    if slow_adapters:
        print("### üêå Slow Adapters (>2s)")
        for test_name, timing in slow_adapters:
            print(f"- **{test_name}**: {timing:.0f}ms")

            if "reddit" in test_name:
                print("  - Consider caching Reddit API responses")
                print("  - Implement concurrent processing for multiple subreddits")
                print("  - Add request batching")
            elif "twelvedata" in test_name:
                print("  - Check API response time vs rate limits")
                print("  - Consider upgrading to premium tier")
                print("  - Implement intelligent fallback ordering")
            elif "twitter" in test_name:
                print("  - Review Twitter API v2 performance")
                print("  - Consider reducing search scope")
                print("  - Implement streaming for real-time data")
        print()

    # Sentiment analysis optimization
    sentiment_tests = [test for test in orchestrator_tests if "sentiment" in test[0]]
    if sentiment_tests and sentiment_tests[0][1] > 5000:
        print("### üìä Sentiment Analysis Optimization")
        print(f"- Current sentiment processing: {sentiment_tests[0][1]:.0f}ms")
        print("- **Recommended optimizations:**")
        print("  - Implement parallel sentiment processing")
        print("  - Add sentiment caching with appropriate TTL")
        print("  - Consider pre-processing popular tickers")
        print("  - Optimize VADER sentiment model performance")
        print()

    # Fast performers to highlight
    fast_adapters = [test for test in adapter_tests if test[1] < 500]
    if fast_adapters:
        print("### üöÄ High-Performance Adapters (<500ms)")
        for test_name, timing in fast_adapters:
            print(f"- **{test_name}**: {timing:.0f}ms - Excellent performance!")
        print()

    # Overall system health
    total_tests = len([r for r in test_results if r.get("status") == "PASS"])
    print("## System Health Summary")
    print(f"- ‚úÖ **Tests Passing**: {total_tests}/22")
    print(f"- üöÄ **Fast Adapters**: {len(fast_adapters)} adapters under 500ms")
    print(f"- üêå **Slow Adapters**: {len(slow_adapters)} adapters over 2s")

    # Calculate average performance
    all_timings = [test[1] for test in adapter_tests + orchestrator_tests + cli_tests]
    avg_timing = sum(all_timings) / len(all_timings)
    print(f"- üìä **Average Response Time**: {avg_timing:.0f}ms")

    if avg_timing < 1000:
        print("- üéØ **Overall Performance**: Excellent system performance!")
    elif avg_timing < 2000:
        print("- üéØ **Overall Performance**: Good system performance")
    else:
        print("- üéØ **Overall Performance**: Consider optimization opportunities")


if __name__ == "__main__":
    analyze_performance()
