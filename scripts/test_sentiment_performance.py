#!/usr/bin/env python3
"""
Performance Test Script for Enhanced Sentiment Pipeline
Tests the optimization improvements to ensure processing time is reduced from 6+ seconds to <2 seconds
"""

import time
import sys
import os
from typing import List, Dict, Any
import statistics

# Add the project root to the path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from data_feeds.enhanced_sentiment_pipeline import get_enhanced_sentiment_pipeline

def run_performance_test(symbols: List[str], num_iterations: int = 3, include_reddit: bool = True) -> Dict[str, Any]:
    """
    Run comprehensive performance test for the sentiment pipeline

    Args:
        symbols: List of stock symbols to test
        num_iterations: Number of iterations to run for each symbol
        include_reddit: Whether to include Reddit sentiment

    Returns:
        Performance test results
    """
    print("üöÄ Starting Enhanced Sentiment Pipeline Performance Test")
    print("=" * 60)

    pipeline = get_enhanced_sentiment_pipeline()

    # Show pipeline configuration
    health = pipeline.get_health_status()
    print(f"üìä Pipeline Configuration:")
    print(f"   ‚Ä¢ Sources: {health['sources_count']}")
    print(f"   ‚Ä¢ Max Workers: {health['max_workers']}")
    print(f"   ‚Ä¢ Timeout: {health['timeout_seconds']}s")
    print(f"   ‚Ä¢ Batch Size: {health['batch_size']}")
    print(f"   ‚Ä¢ Reddit Available: {health['reddit_available']}")
    print()

    all_results = []
    all_times = []

    # Test each symbol multiple times
    for symbol in symbols:
        print(f"üìà Testing {symbol}...")
        symbol_times = []

        for i in range(num_iterations):
            print(f"   Iteration {i+1}/{num_iterations}...", end=" ")

            start_time = time.time()
            try:
                result = pipeline.get_sentiment_analysis(symbol, include_reddit=include_reddit)
                end_time = time.time()
                processing_time = end_time - start_time

                symbol_times.append(processing_time)
                all_times.append(processing_time)

                # Show result summary
                sources = result.get('sources_count', 0)
                sentiment = result.get('overall_sentiment', 0.0)
                confidence = result.get('confidence', 0.0)
                cached = result.get('cached', False)

                print(f"‚úÖ {processing_time:.2f}s "
                      f"({sources} sources, sentiment: {sentiment:.3f}, "
                      f"confidence: {confidence:.3f})"
                      f"{' [CACHED]' if cached else ''}")

                all_results.append({
                    'symbol': symbol,
                    'iteration': i + 1,
                    'processing_time': processing_time,
                    'sources_count': sources,
                    'sentiment': sentiment,
                    'confidence': confidence,
                    'cached': cached
                })

            except Exception as e:
                end_time = time.time()
                processing_time = end_time - start_time
                symbol_times.append(processing_time)
                all_times.append(processing_time)

                print(f"‚ùå {processing_time:.2f}s (Error: {e})")

                all_results.append({
                    'symbol': symbol,
                    'iteration': i + 1,
                    'processing_time': processing_time,
                    'error': str(e)
                })

        # Show symbol summary
        if symbol_times:
            avg_time = statistics.mean(symbol_times)
            min_time = min(symbol_times)
            max_time = max(symbol_times)
            print(f"   üìä {symbol} Summary: Avg={avg_time:.2f}s, Min={min_time:.2f}s, Max={max_time:.2f}s")
            print()

    # Overall performance analysis
    print("üéØ PERFORMANCE ANALYSIS")
    print("=" * 60)

    if all_times:
        avg_time = statistics.mean(all_times)
        median_time = statistics.median(all_times)
        min_time = min(all_times)
        max_time = max(all_times)

        print(f"üìà Overall Performance:")
        print(f"   ‚Ä¢ Average Time: {avg_time:.2f}s")
        print(f"   ‚Ä¢ Median Time: {median_time:.2f}s")
        print(f"   ‚Ä¢ Fastest: {min_time:.2f}s")
        print(f"   ‚Ä¢ Slowest: {max_time:.2f}s")
        print(f"   ‚Ä¢ Total Requests: {len(all_times)}")

        # Performance targets
        target_under_2s = sum(1 for t in all_times if t < 2.0)
        target_under_1s = sum(1 for t in all_times if t < 1.0)

        print(f"üéØ Target Achievement:")
        print(f"   ‚Ä¢ Under 2 seconds: {target_under_2s}/{len(all_times)} ({target_under_2s/len(all_times)*100:.1f}%)")
        print(f"   ‚Ä¢ Under 1 second: {target_under_1s}/{len(all_times)} ({target_under_1s/len(all_times)*100:.1f}%)")

        # Performance rating
        if avg_time < 1.0:
            rating = "üöÄ EXCELLENT"
            color = "‚úÖ"
        elif avg_time < 2.0:
            rating = "üëç VERY GOOD"
            color = "‚úÖ"
        elif avg_time < 3.0:
            rating = "üëå GOOD"
            color = "‚úÖ"
        elif avg_time < 4.0:
            rating = "‚ö†Ô∏è  NEEDS IMPROVEMENT"
            color = "‚ö†Ô∏è"
        else:
            rating = "‚ùå POOR PERFORMANCE"
            color = "‚ùå"

        print(f"   ‚Ä¢ Performance Rating: {color} {rating}")
        print(f"   ‚Ä¢ Improvement from 6+ seconds: {((6.0 - avg_time) / 6.0) * 100:.1f}% faster")

        # Success criteria
        success = avg_time < 2.0
        print(f"   ‚Ä¢ Target Met (<2s): {'‚úÖ YES' if success else '‚ùå NO'}")

    # Show pipeline health
    print()
    print("üè• Pipeline Health Status:")
    health = pipeline.get_health_status()
    print(f"   ‚Ä¢ Status: {health['pipeline_status']}")
    print(f"   ‚Ä¢ Cache Size: {health['cache_size']}")
    print(f"   ‚Ä¢ Performance Stats: {health['performance_stats']}")

    return {
        'results': all_results,
        'performance_stats': {
            'average_time': statistics.mean(all_times) if all_times else 0,
            'median_time': statistics.median(all_times) if all_times else 0,
            'min_time': min(all_times) if all_times else 0,
            'max_time': max(all_times) if all_times else 0,
            'total_requests': len(all_times),
            'target_met': statistics.mean(all_times) < 2.0 if all_times else False
        },
        'pipeline_health': health
    }

def main():
    """Main function to run performance tests"""

    # Test symbols - mix of popular and less common stocks
    test_symbols = ["AAPL", "TSLA", "NVDA", "MSFT", "GOOGL", "META", "AMZN"]

    print("Performance Test Configuration:")
    print(f"  ‚Ä¢ Test Symbols: {test_symbols}")
    print(f"  ‚Ä¢ Iterations per Symbol: 3")
    print(f"  ‚Ä¢ Include Reddit: True")
    print(f"  ‚Ä¢ Target Performance: <2 seconds")
    print()

    # Run the performance test
    results = run_performance_test(
        symbols=test_symbols,
        num_iterations=3,
        include_reddit=True
    )

    # Save results to file
    import json
    with open("sentiment_performance_results.json", "w") as f:
        json.dump(results, f, indent=2, default=str)

    print()
    print("üìÑ Results saved to: sentiment_performance_results.json")

    # Final summary
    stats = results['performance_stats']
    if stats['target_met']:
        print("üéâ SUCCESS: Performance target achieved! Processing time reduced to <2 seconds.")
    else:
        print(f"‚ö†Ô∏è  WARNING: Target not fully achieved. Average time: {stats['average_time']:.2f}s")

    return stats['target_met']

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)