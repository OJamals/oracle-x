"""
Oracle-X Enhanced Options Prediction Pipeline
Advanced ML-driven system with sophisticated feature engineering and risk management.

Key Enhancements:
- Safe mode initialization bypassing configuration issues
- Advanced technical indicators and volatility modeling
- Enhanced ML ensemble with multiple algorithms
- Options-specific analytics (Greeks, IV surfaces, flow analysis)
- Comprehensive risk management with VaR and stress testing
- Real-time model adaptation and performance monitoring
"""

import logging
import time
import json
import signal
import warnings
from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError as FuturesTimeoutError
from dataclasses import dataclass, field, asdict
from datetime import datetime, timedelta
from enum import Enum
from typing import Dict, List, Optional, Tuple, Any, Union, Protocol
from typing_extensions import runtime_checkable
from functools import lru_cache
import threading

import numpy as np
import pandas as pd
import scipy.stats as stats
from scipy.optimize import minimize
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import cross_val_score, TimeSeriesSplit
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
import talib

# Suppress warnings for cleaner output
warnings.filterwarnings('ignore')

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# ===================================================================
# SHARED DATA STRUCTURES AND ENUMS
# ===================================================================

class OptionType(Enum):
    """Option type enumeration"""
    CALL = "call"
    PUT = "put"


class OptionStyle(Enum):
    """Option exercise style"""
    EUROPEAN = "european"
    AMERICAN = "american"


@dataclass
class OptionContract:
    """Option contract data structure"""
    symbol: str
    strike: float
    expiry: datetime
    option_type: OptionType
    style: OptionStyle = OptionStyle.AMERICAN
    bid: Optional[float] = None
    ask: Optional[float] = None
    last: Optional[float] = None
    volume: Optional[int] = None
    open_interest: Optional[int] = None
    implied_volatility: Optional[float] = None
    underlying_price: Optional[float] = None
    
    @property
    def market_price(self) -> Optional[float]:
        """Get the best available market price"""
        if self.bid and self.ask:
            return (self.bid + self.ask) / 2
        return self.last


@dataclass
class ValuationResult:
    """Comprehensive valuation result"""
    contract: OptionContract
    theoretical_value: float
    market_price: float
    mispricing: float  # theoretical - market
    mispricing_ratio: float  # (theoretical - market) / market
    confidence_score: float
    pricing_breakdown: Dict[str, float]
    greeks: Dict[str, float]
    implied_volatility: Optional[float] = None
    historical_volatility: Optional[float] = None
    iv_rank: Optional[float] = None  # IV percentile over past year
    iv_skew: Optional[float] = None
    
    @property
    def is_undervalued(self) -> bool:
        """Check if option is undervalued"""
        return self.mispricing > 0 and self.mispricing_ratio > 0.05
    
    @property
    def opportunity_score(self) -> float:
        """Calculate opportunity score (0-100)"""
        score = 0.0
        
        # Mispricing component (40%)
        if self.is_undervalued:
            mispricing_score = min(40, self.mispricing_ratio * 100)
            score += mispricing_score
        
        # Confidence component (30%)
        score += self.confidence_score * 0.3
        
        # Liquidity component (20%)
        if self.contract.volume and self.contract.open_interest:
            liquidity_score = min(20, 
                (min(self.contract.volume / 100, 1.0) * 10 +
                 min(self.contract.open_interest / 1000, 1.0) * 10))
            score += liquidity_score
        
        # Greeks favorability (10%)
        if self.greeks:
            # Favorable delta for the position
            delta_score = abs(self.greeks.get('delta', 0)) * 10
            score += min(10, delta_score)
        
        return min(100, score)


# ===================================================================
# SHARED COMPONENTS (USED BY BOTH STANDARD AND ENHANCED PIPELINES)
# ===================================================================

class RiskTolerance(Enum):
    """Risk tolerance levels for position sizing"""
    CONSERVATIVE = "conservative"
    MODERATE = "moderate"
    AGGRESSIVE = "aggressive"


class OptionStrategy(Enum):
    """Supported option strategies"""
    LONG_CALL = "long_call"
    LONG_PUT = "long_put"
    CALL_SPREAD = "call_spread"
    PUT_SPREAD = "put_spread"
    STRADDLE = "straddle"
    STRANGLE = "strangle"


@dataclass
class PipelineConfig:
    """Configuration for the standard options pipeline"""
    # Risk management
    risk_tolerance: RiskTolerance = RiskTolerance.MODERATE
    max_position_size: float = 0.05  # 5% of portfolio
    min_opportunity_score: float = 70.0
    min_confidence: float = 0.6
    
    # Strategy preferences
    preferred_strategies: List[OptionStrategy] = field(default_factory=lambda: [
        OptionStrategy.LONG_CALL, 
        OptionStrategy.LONG_PUT
    ])
    
    # Time preferences
    min_days_to_expiry: int = 7
    max_days_to_expiry: int = 90
    
    # Liquidity requirements
    min_volume: int = 100
    min_open_interest: int = 500
    max_spread_ratio: float = 0.05
    
    # Processing
    max_workers: int = 4
    cache_ttl: int = 300  # 5 minutes
    max_options_per_symbol: int = 40  # Performance guard
    per_symbol_timeout: int = 10  # Seconds
    orchestrator_init_timeout: int = 10  # Seconds
    safe_mode: bool = False  # If True, use stub orchestrator / lightweight path
    
    # Data sources
    use_advanced_sentiment: bool = True
    use_options_flow: bool = True
    use_market_internals: bool = True


@dataclass
class OptionRecommendation:
    """Detailed option trade recommendation"""
    # Contract details
    symbol: str
    contract: OptionContract
    strategy: OptionStrategy
    
    # Scoring
    opportunity_score: float  # 0-100
    ml_confidence: float  # 0-1
    valuation_score: float  # Mispricing ratio
    
    # Trade parameters
    entry_price: float
    target_price: float
    stop_loss: float
    position_size: float  # As % of portfolio
    max_contracts: int
    
    # Risk metrics
    max_loss: float
    expected_return: float
    probability_of_profit: float
    risk_reward_ratio: float
    breakeven_price: float
    
    # Analysis
    key_reasons: List[str]
    risk_factors: List[str]
    entry_signals: List[str]
    
    # Metadata
    timestamp: datetime
    data_quality: float
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert recommendation to dictionary"""
        return {
            'symbol': self.symbol,
            'contract': {
                'strike': self.contract.strike,
                'expiry': self.contract.expiry.isoformat(),
                'type': self.contract.option_type.value,
                'bid': float(self.contract.bid) if self.contract.bid else None,
                'ask': float(self.contract.ask) if self.contract.ask else None,
                'volume': self.contract.volume,
                'open_interest': self.contract.open_interest
            },
            'strategy': self.strategy.value,
            'scores': {
                'opportunity': round(self.opportunity_score, 2),
                'ml_confidence': round(self.ml_confidence, 3),
                'valuation': round(self.valuation_score, 3)
            },
            'trade': {
                'entry_price': round(self.entry_price, 2),
                'target_price': round(self.target_price, 2),
                'stop_loss': round(self.stop_loss, 2),
                'position_size': round(self.position_size, 3),
                'max_contracts': self.max_contracts
            },
            'risk': {
                'max_loss': round(self.max_loss, 2),
                'expected_return': round(self.expected_return, 3),
                'probability_of_profit': round(self.probability_of_profit, 3),
                'risk_reward_ratio': round(self.risk_reward_ratio, 2),
                'breakeven_price': round(self.breakeven_price, 2)
            },
            'analysis': {
                'key_reasons': self.key_reasons,
                'risk_factors': self.risk_factors,
                'entry_signals': self.entry_signals
            },
            'metadata': {
                'timestamp': self.timestamp.isoformat(),
                'data_quality': round(self.data_quality, 2)
            }
        }


@dataclass
class PipelineResult:
    """Result from pipeline execution"""
    recommendations: List[OptionRecommendation]
    execution_time: float
    symbols_analyzed: int
    opportunities_found: int
    errors: List[str]
    warnings: List[str]
    timestamp: datetime


@runtime_checkable
class OrchestratorProtocol(Protocol):
    def get_market_data(self, *args, **kwargs): ...  # pragma: no cover
    def get_options_analytics(self, *args, **kwargs): ...  # pragma: no cover
    def get_quote(self, *args, **kwargs): ...  # pragma: no cover


class BaseOptionsPipeline:
    """Base class with shared functionality for all option pipelines"""
    
    def __init__(self, config):
        """Initialize base pipeline components"""
        self.config = config
        self.executor = ThreadPoolExecutor(max_workers=config.max_workers)
        self._cache = {}
        self._cache_timestamps = {}
        self._market_data_cache = {}
        self._options_chain_cache = {}
        self._cache_lock = threading.Lock()
        
        # Initialize orchestrator with timeout protection
        self.orchestrator = self._init_orchestrator_with_timeout()
        
        # Initialize valuation engine
        self.valuation_engine = OptionsValuationEngine(
            cache_ttl=config.cache_ttl,
            confidence_threshold=config.min_confidence,
            max_workers=config.max_workers
        )
        
        # Signal aggregator
        self.signal_aggregator = SignalAggregator(self.orchestrator)
    
    class _SafeStubOrchestrator:
        """Lightweight stub to allow pipeline operation in safe mode"""
        def get_market_data(self, *_, **__):
            return None
        def get_options_analytics(self, *_, **__):
            return {}
        def get_quote(self, *_, **__):
            return None

    def _init_orchestrator_with_timeout(self):
        """Initialize DataFeedOrchestrator with timeout & safe fallback"""
        if self.config.safe_mode:
            logger.warning("Safe mode enabled: using stub orchestrator")
            return self._SafeStubOrchestrator()
        
        orchestrator_holder = {}
        exc_holder = {}

        def _init():
            try:
                orchestrator_holder['obj'] = DataFeedOrchestrator()
            except Exception as e:
                exc_holder['err'] = e

        t = threading.Thread(target=_init, daemon=True)
        t.start()
        t.join(timeout=self.config.orchestrator_init_timeout)
        
        if t.is_alive():
            logger.error("DataFeedOrchestrator initialization timed out; falling back to stub.")
            return self._SafeStubOrchestrator()
        if 'err' in exc_holder:
            logger.error(f"DataFeedOrchestrator initialization failed: {exc_holder['err']} - using stub")
            return self._SafeStubOrchestrator()
        return orchestrator_holder.get('obj', self._SafeStubOrchestrator())
    
    def shutdown(self):
        """Clean shutdown of the pipeline"""
        logger.info("Shutting down pipeline...")
        self.valuation_engine.clear_cache()
        self._cache.clear()
        self.executor.shutdown(wait=True)
        logger.info("Pipeline shutdown complete")


# ===================================================================
# STANDARD PIPELINE IMPLEMENTATION  
# ===================================================================

class OracleOptionsPipeline(BaseOptionsPipeline):
    """
    Standard Oracle Options Pipeline
    Provides core functionality for options opportunity identification
    """
    
    def __init__(self, config: Optional[PipelineConfig] = None):
        """Initialize the Standard Oracle Options Pipeline"""
        self.config = config or PipelineConfig()
        super().__init__(self.config)
        
        # Initialize ML prediction model (lazy loading)
        self.prediction_model = None
        self._init_prediction_model()
        
        logger.info("Standard Oracle Options Pipeline initialized successfully")
    
    def _init_prediction_model(self):
        """Initialize ML prediction model (lazy loading)"""
        try:
            from oracle_engine.ensemble_ml_engine import EnsemblePredictionEngine
            from data_feeds.advanced_sentiment import AdvancedSentimentEngine
            
            sentiment_engine = AdvancedSentimentEngine()
            ensemble_engine = EnsemblePredictionEngine(self.orchestrator, sentiment_engine)
            self.prediction_model = OptionsPredictionModel(ensemble_engine, self.orchestrator)
            logger.info("ML prediction model initialized")
        except Exception as e:
            logger.warning(f"ML model initialization failed: {e}. Using fallback scoring.")
            self.prediction_model = None

    def analyze_ticker(self, symbol: str, expiry: Optional[str] = None, use_cache: bool = True) -> List[OptionRecommendation]:
        """Analyze a single ticker for opportunities"""
        logger.info(f"Analyzing {symbol}...")
        
        # Check cache
        cache_key = f"{symbol}_{expiry or 'nearest'}"
        if use_cache and cache_key in self._cache:
            cache_time = self._cache_timestamps.get(cache_key, 0)
            if time.time() - cache_time < self.config.cache_ttl:
                logger.info(f"Using cached results for {symbol}")
                return self._cache[cache_key]
        
        recommendations = []
        
        try:
            # Step 1: Get market data
            market_data = self._fetch_market_data(symbol)
            if market_data is None or market_data.empty:
                logger.warning(f"No market data available for {symbol}")
                return []
            
            # Step 2: Get options chain
            options_chain = self._fetch_options_chain(symbol, expiry)
            if not options_chain:
                logger.warning(f"No options chain available for {symbol}")
                return []
            
            # Step 3: Filter options and analyze
            filtered_options = self._filter_options(options_chain)
            if not filtered_options:
                return []
            
            # Performance guard: limit number of options per symbol
            if len(filtered_options) > self.config.max_options_per_symbol:
                underlying = filtered_options[0].underlying_price or 0
                filtered_options.sort(key=lambda c: abs((c.strike or 0) - underlying))
                filtered_options = filtered_options[:self.config.max_options_per_symbol]
            
            logger.info(f"Analyzing {len(filtered_options)} options for {symbol}")
            
            # Step 4: Parallel analysis of each option
            analysis_workers = min(self.config.max_workers, max(1, len(filtered_options)))
            with ThreadPoolExecutor(max_workers=analysis_workers) as executor:
                futures = {
                    executor.submit(self._analyze_single_option, symbol, option, market_data): option
                    for option in filtered_options
                }
                for future in as_completed(futures):
                    try:
                        rec = future.result(timeout=self.config.per_symbol_timeout)
                        if rec:
                            recommendations.append(rec)
                    except Exception as e:
                        logger.error(f"Failed to analyze option: {e}")
            
            # Sort by opportunity score
            recommendations.sort(key=lambda x: x.opportunity_score, reverse=True)
            
            # Cache results
            self._cache[cache_key] = recommendations
            self._cache_timestamps[cache_key] = time.time()
            
            logger.info(f"Found {len(recommendations)} opportunities for {symbol}")
            
        except Exception as e:
            logger.error(f"Error analyzing {symbol}: {e}")
        
        return recommendations
    
    def generate_market_scan(self, symbols: List[str], **kwargs) -> Dict[str, Any]:
        """Generate market scan across multiple symbols"""
        start_time = time.time()
        logger.info(f"Scanning {len(symbols)} symbols for opportunities...")
        
        all_recommendations = []
        errors = []
        
        # Process symbols in parallel batches
        batch_size = 10
        for i in range(0, len(symbols), batch_size):
            batch = symbols[i:i+batch_size]
            
            with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:
                futures = {executor.submit(self.analyze_ticker, symbol): symbol for symbol in batch}
                
                for future in as_completed(futures):
                    symbol = futures[future]
                    try:
                        recommendations = future.result()
                        all_recommendations.extend(recommendations)
                    except Exception as e:
                        error_msg = f"Failed to analyze {symbol}: {e}"
                        logger.error(error_msg)
                        errors.append(error_msg)
        
        # Sort by opportunity score
        all_recommendations.sort(key=lambda x: x.opportunity_score, reverse=True)
        
        execution_time = time.time() - start_time
        
        result = {
            'recommendations': all_recommendations[:20],  # Top 20
            'top_opportunities': all_recommendations[:10],  # Top 10
            'scan_results': {
                'symbols_scanned': len(symbols),
                'opportunities_found': len(all_recommendations),
                'execution_time': execution_time,
                'errors': len(errors)
            }
        }
        
        logger.info(f"Market scan complete: {len(all_recommendations)} opportunities found in {execution_time:.2f}s")
        return result

    def _fetch_market_data(self, symbol: str) -> Optional[pd.DataFrame]:
        """Fetch market data for analysis"""
        now = time.time()
        with self._cache_lock:
            cached = self._market_data_cache.get(symbol)
            if cached and (now - cached[0] < self.config.cache_ttl):
                return cached[1]
        try:
            market_data = self.orchestrator.get_market_data(symbol, period="1mo", interval="1d")
            if market_data and getattr(market_data, 'data', None) is not None and not market_data.data.empty:
                with self._cache_lock:
                    self._market_data_cache[symbol] = (now, market_data.data)
                return market_data.data
        except Exception as e:
            logger.error(f"Failed to fetch market data for {symbol}: {e}")
        return None
    
    def _fetch_options_chain(self, symbol: str, expiry: Optional[str] = None) -> List[OptionContract]:
        """Fetch and parse options chain"""
        cache_key = f"{symbol}:{expiry or 'ALL'}"
        now = time.time()
        with self._cache_lock:
            cached = self._options_chain_cache.get(cache_key)
            if cached and (now - cached[0] < self.config.cache_ttl):
                return cached[1]
        options = []
        try:
            analytics = self.orchestrator.get_options_analytics(symbol, include=['chain', 'iv', 'greeks'])
            if not analytics or 'chain' not in analytics:
                return []
            chain = analytics.get('chain', [])
            for opt in chain:
                try:
                    exp_str = opt.get('expiry')
                    if not exp_str:
                        continue
                    if expiry and exp_str != expiry:
                        continue
                    exp_date = datetime.strptime(exp_str, '%Y-%m-%d')
                    contract = OptionContract(
                        symbol=symbol,
                        strike=float(opt.get('strike', 0)),
                        expiry=exp_date,
                        option_type=OptionType.CALL if opt.get('put_call') == 'call' else OptionType.PUT,
                        bid=float(opt.get('bid', 0)) if opt.get('bid') else None,
                        ask=float(opt.get('ask', 0)) if opt.get('ask') else None,
                        last=float(opt.get('last', 0)) if opt.get('last') else None,
                        volume=int(opt.get('volume', 0)) if opt.get('volume') else None,
                        open_interest=int(opt.get('open_interest', 0)) if opt.get('open_interest') else None,
                        underlying_price=float(opt.get('underlying', 0))
                    )
                    options.append(contract)
                except Exception as e:
                    logger.debug(f"Failed to parse option: {e}")
                    continue
            with self._cache_lock:
                self._options_chain_cache[cache_key] = (now, options)
        except Exception as e:
            logger.error(f"Failed to fetch options chain for {symbol}: {e}")
        return options
    
    def _filter_options(self, options: List[OptionContract]) -> List[OptionContract]:
        """Filter options based on configuration criteria"""
        filtered = []
        
        for option in options:
            # Check days to expiry
            days_to_expiry = option.time_to_expiry * 365
            if days_to_expiry < self.config.min_days_to_expiry:
                continue
            if days_to_expiry > self.config.max_days_to_expiry:
                continue
            
            # Check liquidity
            if option.volume and option.volume < self.config.min_volume:
                continue
            if option.open_interest and option.open_interest < self.config.min_open_interest:
                continue
            
            # Check spread
            if option.bid and option.ask:
                spread_ratio = (option.ask - option.bid) / option.ask
                if spread_ratio > self.config.max_spread_ratio:
                    continue
            
            filtered.append(option)
        
        return filtered
    
    def _analyze_single_option(self, symbol: str, contract: OptionContract, market_data: pd.DataFrame) -> Optional[OptionRecommendation]:
        """Analyze a single option for opportunity"""
        try:
            # Step 1: Valuation analysis
            valuation = self.valuation_engine.detect_mispricing(
                contract,
                contract.underlying_price or 100.0,
                market_data
            )
            
            if not valuation:
                return None
            
            # Skip if not undervalued enough
            if valuation.opportunity_score < self.config.min_opportunity_score:
                return None
            
            # Step 2: ML prediction (if available)
            ml_confidence = 0.5
            expected_return = valuation.mispricing_ratio
            
            if self.prediction_model:
                try:
                    prediction = self.prediction_model.predict(symbol, contract, lookback_days=30)
                    confidence_mapping = {'high': 0.8, 'medium': 0.6, 'low': 0.4}
                    ml_confidence = confidence_mapping.get(prediction.confidence.value, 0.5)
                    expected_return = prediction.expected_return
                except Exception as e:
                    logger.debug(f"ML prediction failed: {e}")
            
            # Step 3: Calculate metrics and create recommendation
            opportunity_score = self._calculate_opportunity_score(valuation, ml_confidence, expected_return)
            
            if opportunity_score < self.config.min_opportunity_score:
                return None
            
            position_size = self._calculate_position_size(opportunity_score, ml_confidence, valuation)
            
            # Trade parameters
            bid = contract.bid or 0.0
            ask = contract.ask or 0.0
            entry_price = contract.market_price or contract.last or (bid + ask) / 2
            
            # Target based on expected move
            underlying_price = contract.underlying_price or 100.0
            if contract.option_type == OptionType.CALL:
                target_underlying = underlying_price * (1 + abs(expected_return))
                target_price = max(0, target_underlying - contract.strike)
            else:
                target_underlying = underlying_price * (1 - abs(expected_return))
                target_price = max(0, contract.strike - target_underlying)
            
            stop_loss = entry_price * 0.7
            max_loss = entry_price * position_size
            risk_reward = (target_price - entry_price) / (entry_price - stop_loss) if stop_loss < entry_price else 0
            
            # Breakeven calculation
            if contract.option_type == OptionType.CALL:
                breakeven = contract.strike + entry_price
            else:
                breakeven = contract.strike - entry_price
            
            # Key reasons and risk factors
            key_reasons = []
            if valuation.is_undervalued:
                key_reasons.append(f"Undervalued by {valuation.mispricing_ratio:.1%}")
            if ml_confidence > 0.7:
                key_reasons.append(f"High ML confidence: {ml_confidence:.1%}")
            if valuation.iv_rank and valuation.iv_rank < 30:
                key_reasons.append(f"Low IV rank: {valuation.iv_rank:.0f}")
            
            risk_factors = []
            if contract.time_to_expiry < 0.08:
                risk_factors.append("High time decay risk")
            if valuation.implied_volatility and valuation.implied_volatility > 0.5:
                risk_factors.append("High volatility")
            if contract.volume and contract.volume < 500:
                risk_factors.append("Lower liquidity")
            
            # Create recommendation
            recommendation = OptionRecommendation(
                symbol=symbol,
                contract=contract,
                strategy=OptionStrategy.LONG_CALL if contract.option_type == OptionType.CALL else OptionStrategy.LONG_PUT,
                opportunity_score=opportunity_score,
                ml_confidence=ml_confidence,
                valuation_score=valuation.mispricing_ratio,
                entry_price=entry_price,
                target_price=target_price,
                stop_loss=stop_loss,
                position_size=position_size,
                max_contracts=int(position_size * 10000 / (entry_price * 100)),
                max_loss=max_loss,
                expected_return=expected_return,
                probability_of_profit=ml_confidence,
                risk_reward_ratio=risk_reward,
                breakeven_price=breakeven,
                key_reasons=key_reasons,
                risk_factors=risk_factors,
                entry_signals=[],
                timestamp=datetime.now(),
                data_quality=valuation.confidence_score
            )
            
            return recommendation
            
        except Exception as e:
            logger.error(f"Failed to analyze option: {e}")
            return None
    
    def _calculate_opportunity_score(self, valuation: ValuationResult, ml_confidence: float, expected_return: float) -> float:
        """Calculate composite opportunity score"""
        # Weighted scoring
        valuation_weight = 0.4
        ml_weight = 0.3
        return_weight = 0.3
        
        valuation_score = valuation.opportunity_score
        ml_score = ml_confidence * 100
        return_score = min(100, abs(expected_return) * 200)
        
        score = (
            valuation_score * valuation_weight +
            ml_score * ml_weight +
            return_score * return_weight
        )
        
        return min(100, score)
    
    def _calculate_position_size(self, opportunity_score: float, ml_confidence: float, valuation: ValuationResult) -> float:
        """Calculate position size based on Kelly Criterion and risk tolerance"""
        base_size = self.config.max_position_size
        
        # Adjust based on risk tolerance
        if self.config.risk_tolerance == RiskTolerance.CONSERVATIVE:
            base_size *= 0.5
        elif self.config.risk_tolerance == RiskTolerance.AGGRESSIVE:
            base_size *= 1.5
        
        # Scale by opportunity score and confidence
        score_factor = opportunity_score / 100
        confidence_factor = (ml_confidence + valuation.confidence_score / 100) / 2
        
        position_size = base_size * score_factor * confidence_factor
        
        # Apply Kelly Criterion cap
        kelly_fraction = confidence_factor - (1 - confidence_factor)
        kelly_size = max(0, min(0.25, kelly_fraction * 0.25))
        
        return min(position_size, kelly_size, self.config.max_position_size)


# Factory function for standard pipeline
def create_pipeline(config: Optional[Dict[str, Any]] = None) -> OracleOptionsPipeline:
    """Create and initialize a Standard Oracle Options Pipeline"""
    if config:
        # Convert dict to PipelineConfig
        risk_tolerance = config.get('risk_tolerance', 'moderate')
        if isinstance(risk_tolerance, str):
            risk_tolerance = RiskTolerance(risk_tolerance)
        
        strategies = config.get('preferred_strategies', ['long_call', 'long_put'])
        if strategies and isinstance(strategies[0], str):
            strategies = [OptionStrategy(s) for s in strategies]
        
        pipeline_config = PipelineConfig(
            risk_tolerance=risk_tolerance,
            max_position_size=config.get('max_position_size', 0.05),
            min_opportunity_score=config.get('min_opportunity_score', 70.0),
            min_confidence=config.get('min_confidence', 0.6),
            preferred_strategies=strategies,
            min_days_to_expiry=config.get('min_days_to_expiry', 7),
            max_days_to_expiry=config.get('max_days_to_expiry', 90),
            min_volume=config.get('min_volume', 100),
            min_open_interest=config.get('min_open_interest', 500),
            max_spread_ratio=config.get('max_spread_ratio', 0.05),
            max_workers=config.get('max_workers', 4),
            cache_ttl=config.get('cache_ttl', 300),
            use_advanced_sentiment=config.get('use_advanced_sentiment', True),
            use_options_flow=config.get('use_options_flow', True),
            use_market_internals=config.get('use_market_internals', True)
        )
    else:
        pipeline_config = PipelineConfig()
    
    return OracleOptionsPipeline(pipeline_config)


# ===================================================================
# ENHANCED PIPELINE COMPONENTS (PRESERVED FROM ORIGINAL)
# ===================================================================
    """Safe mode operation levels"""
    FULL = "full"  # All features enabled, may have initialization issues
    SAFE = "safe"  # Bypass problematic components, use fallbacks
    MINIMAL = "minimal"  # Only essential features, maximum stability


class ModelComplexity(Enum):
    """Model complexity levels for performance tuning"""
    SIMPLE = "simple"
    MODERATE = "moderate"  
    ADVANCED = "advanced"
    RESEARCH = "research"


class OptionStrategy(Enum):
    """Supported option strategies"""
    LONG_CALL = "long_call"
    LONG_PUT = "long_put"
    CALL_SPREAD = "call_spread"
    PUT_SPREAD = "put_spread"
    STRADDLE = "straddle"
    STRANGLE = "strangle"


class SafeMode(Enum):
    """Safe mode operations for enhanced pipeline"""
    FULL = "full"          # Full functionality
    SAFE = "safe"          # Safe operations only
    MINIMAL = "minimal"    # Minimal operations


class ModelComplexity(Enum):
    """Model complexity levels"""
    SIMPLE = "simple"
    MODERATE = "moderate"
    ADVANCED = "advanced"


@dataclass
class EnhancedPipelineConfig:
    """Enhanced configuration for the options pipeline"""
    # Operation mode
    safe_mode: SafeMode = SafeMode.SAFE
    model_complexity: ModelComplexity = ModelComplexity.MODERATE
    enable_advanced_features: bool = True
    enable_backtesting: bool = False
    
    # Risk management
    risk_tolerance: RiskTolerance = RiskTolerance.MODERATE
    max_position_size: float = 0.05  # 5% of portfolio
    min_opportunity_score: float = 70.0
    min_confidence: float = 0.6
    enable_var_calculation: bool = True
    enable_stress_testing: bool = True
    
    # Strategy preferences
    preferred_strategies: List[OptionStrategy] = field(default_factory=lambda: [
        OptionStrategy.LONG_CALL, 
        OptionStrategy.LONG_PUT
    ])
    
    # Time preferences
    min_days_to_expiry: int = 7
    max_days_to_expiry: int = 90
    
    # Liquidity requirements
    min_volume: int = 100
    min_open_interest: int = 500
    max_spread_ratio: float = 0.05
    
    # Processing
    max_workers: int = 4
    cache_ttl: int = 300  # 5 minutes
    orchestrator_init_timeout: int = 5  # seconds
    
    # Feature engineering
    lookback_periods: List[int] = field(default_factory=lambda: [5, 10, 20, 50])
    volatility_models: List[str] = field(default_factory=lambda: ["garch", "realized", "ewma"])
    technical_indicators: List[str] = field(default_factory=lambda: [
        "rsi", "macd", "bollinger", "stochastic", "williams_r", "cci"
    ])
    
    # ML parameters
    ensemble_models: List[str] = field(default_factory=lambda: ["rf", "gbm", "xgb"])
    cross_validation_folds: int = 5
    feature_selection: bool = True
    hyperparameter_optimization: bool = False
    
    # Data sources
    use_advanced_sentiment: bool = True
    use_options_flow: bool = True
    use_market_internals: bool = True
    use_alternative_data: bool = False


@dataclass 
class AdvancedFeatures:
    """Container for advanced technical and fundamental features"""
    # Technical indicators
    rsi: Optional[float] = None
    macd: Optional[float] = None
    macd_signal: Optional[float] = None
    macd_histogram: Optional[float] = None
    bollinger_upper: Optional[float] = None
    bollinger_lower: Optional[float] = None
    bollinger_position: Optional[float] = None
    stochastic_k: Optional[float] = None
    stochastic_d: Optional[float] = None
    williams_r: Optional[float] = None
    cci: Optional[float] = None
    
    # Volatility features
    realized_volatility_5d: Optional[float] = None
    realized_volatility_20d: Optional[float] = None
    garch_volatility: Optional[float] = None
    ewma_volatility: Optional[float] = None
    volatility_ratio: Optional[float] = None
    volatility_regime: Optional[str] = None
    
    # Options-specific features
    iv_rank: Optional[float] = None
    iv_percentile: Optional[float] = None
    put_call_ratio: Optional[float] = None
    options_volume_ratio: Optional[float] = None
    skew: Optional[float] = None
    term_structure_slope: Optional[float] = None
    
    # Market microstructure
    dark_pool_flow: Optional[float] = None
    institutional_flow: Optional[float] = None
    retail_sentiment: Optional[float] = None
    
    # Risk metrics
    var_1d: Optional[float] = None
    var_5d: Optional[float] = None
    max_drawdown: Optional[float] = None
    sharpe_ratio: Optional[float] = None
    sortino_ratio: Optional[float] = None


class EnhancedFeatureEngine:
    """Advanced feature engineering for options prediction"""
    
    def __init__(self, config: EnhancedPipelineConfig):
        self.config = config
        self.scaler = StandardScaler()
        
    def extract_features(self, 
                        symbol: str,
                        market_data: pd.DataFrame,
                        options_data: Optional[Dict] = None) -> AdvancedFeatures:
        """Extract comprehensive features from market data"""
        
        if market_data.empty:
            return AdvancedFeatures()
            
        try:
            features = AdvancedFeatures()
            
            # Price data
            prices = market_data['close'].values
            highs = market_data['high'].values
            lows = market_data['low'].values
            volumes = market_data['volume'].values if 'volume' in market_data else None
            
            # Technical indicators
            if 'rsi' in self.config.technical_indicators:
                features.rsi = self._calculate_rsi(prices)
                
            if 'macd' in self.config.technical_indicators:
                macd_values = self._calculate_macd(prices)
                features.macd = macd_values['macd']
                features.macd_signal = macd_values['signal']
                features.macd_histogram = macd_values['histogram']
                
            if 'bollinger' in self.config.technical_indicators:
                bb_values = self._calculate_bollinger_bands(prices)
                features.bollinger_upper = bb_values['upper']
                features.bollinger_lower = bb_values['lower']
                features.bollinger_position = bb_values['position']
                
            if 'stochastic' in self.config.technical_indicators:
                stoch_values = self._calculate_stochastic(highs, lows, prices)
                features.stochastic_k = stoch_values['k']
                features.stochastic_d = stoch_values['d']
                
            if 'williams_r' in self.config.technical_indicators:
                features.williams_r = self._calculate_williams_r(highs, lows, prices)
                
            if 'cci' in self.config.technical_indicators:
                features.cci = self._calculate_cci(highs, lows, prices)
            
            # Volatility features
            features.realized_volatility_5d = self._calculate_realized_volatility(prices, 5)
            features.realized_volatility_20d = self._calculate_realized_volatility(prices, 20)
            
            if 'garch' in self.config.volatility_models:
                features.garch_volatility = self._estimate_garch_volatility(prices)
                
            if 'ewma' in self.config.volatility_models:
                features.ewma_volatility = self._calculate_ewma_volatility(prices)
                
            features.volatility_ratio = self._calculate_volatility_ratio(prices)
            features.volatility_regime = self._detect_volatility_regime(prices)
            
            # Options-specific features (if options data available)
            if options_data:
                features.iv_rank = self._calculate_iv_rank(options_data)
                features.iv_percentile = self._calculate_iv_percentile(options_data)
                features.put_call_ratio = self._calculate_put_call_ratio(options_data)
                features.skew = self._calculate_options_skew(options_data)
                
            # Risk metrics
            features.var_1d = self._calculate_var(prices, confidence=0.95, horizon=1)
            features.var_5d = self._calculate_var(prices, confidence=0.95, horizon=5)
            features.max_drawdown = self._calculate_max_drawdown(prices)
            features.sharpe_ratio = self._calculate_sharpe_ratio(prices)
            features.sortino_ratio = self._calculate_sortino_ratio(prices)
            
            return features
            
        except Exception as e:
            logger.error(f"Feature extraction failed for {symbol}: {e}")
            return AdvancedFeatures()
    
    def _calculate_rsi(self, prices: np.ndarray, period: int = 14) -> Optional[float]:
        """Calculate RSI using TA-Lib"""
        try:
            if len(prices) < period + 1:
                return None
            rsi = talib.RSI(prices, timeperiod=period)
            return float(rsi[-1]) if not np.isnan(rsi[-1]) else None
        except:
            return None
    
    def _calculate_macd(self, prices: np.ndarray) -> Dict[str, Optional[float]]:
        """Calculate MACD using TA-Lib"""
        try:
            if len(prices) < 35:  # Need enough data for MACD
                return {'macd': None, 'signal': None, 'histogram': None}
                
            macd, signal, histogram = talib.MACD(prices)
            return {
                'macd': float(macd[-1]) if not np.isnan(macd[-1]) else None,
                'signal': float(signal[-1]) if not np.isnan(signal[-1]) else None,
                'histogram': float(histogram[-1]) if not np.isnan(histogram[-1]) else None
            }
        except:
            return {'macd': None, 'signal': None, 'histogram': None}
    
    def _calculate_bollinger_bands(self, prices: np.ndarray, period: int = 20, std_dev: int = 2) -> Dict[str, Optional[float]]:
        """Calculate Bollinger Bands"""
        try:
            if len(prices) < period:
                return {'upper': None, 'lower': None, 'position': None}
                
            upper, middle, lower = talib.BBANDS(prices, timeperiod=period, nbdevup=std_dev, nbdevdn=std_dev)
            
            current_price = prices[-1]
            current_upper = upper[-1]
            current_lower = lower[-1]
            
            # Calculate position within bands (0 = lower band, 1 = upper band)
            if not (np.isnan(current_upper) or np.isnan(current_lower)):
                position = (current_price - current_lower) / (current_upper - current_lower)
            else:
                position = None
                
            return {
                'upper': float(current_upper) if not np.isnan(current_upper) else None,
                'lower': float(current_lower) if not np.isnan(current_lower) else None,
                'position': float(position) if position is not None else None
            }
        except:
            return {'upper': None, 'lower': None, 'position': None}
    
    def _calculate_stochastic(self, highs: np.ndarray, lows: np.ndarray, closes: np.ndarray, period: int = 14) -> Dict[str, Optional[float]]:
        """Calculate Stochastic Oscillator"""
        try:
            if len(closes) < period:
                return {'k': None, 'd': None}
                
            slowk, slowd = talib.STOCH(highs, lows, closes, fastk_period=period)
            return {
                'k': float(slowk[-1]) if not np.isnan(slowk[-1]) else None,
                'd': float(slowd[-1]) if not np.isnan(slowd[-1]) else None
            }
        except:
            return {'k': None, 'd': None}
    
    def _calculate_williams_r(self, highs: np.ndarray, lows: np.ndarray, closes: np.ndarray, period: int = 14) -> Optional[float]:
        """Calculate Williams %R"""
        try:
            if len(closes) < period:
                return None
            willr = talib.WILLR(highs, lows, closes, timeperiod=period)
            return float(willr[-1]) if not np.isnan(willr[-1]) else None
        except:
            return None
    
    def _calculate_cci(self, highs: np.ndarray, lows: np.ndarray, closes: np.ndarray, period: int = 20) -> Optional[float]:
        """Calculate Commodity Channel Index"""
        try:
            if len(closes) < period:
                return None
            cci = talib.CCI(highs, lows, closes, timeperiod=period)
            return float(cci[-1]) if not np.isnan(cci[-1]) else None
        except:
            return None
    
    def _calculate_realized_volatility(self, prices: np.ndarray, period: int) -> Optional[float]:
        """Calculate realized volatility"""
        try:
            if len(prices) < period + 1:
                return None
                
            returns = np.diff(np.log(prices))
            if len(returns) < period:
                return None
                
            recent_returns = returns[-period:]
            volatility = np.std(recent_returns) * np.sqrt(252)  # Annualized
            return float(volatility)
        except:
            return None
    
    def _estimate_garch_volatility(self, prices: np.ndarray) -> Optional[float]:
        """Simplified GARCH volatility estimation"""
        try:
            if len(prices) < 30:
                return None
                
            returns = np.diff(np.log(prices))
            
            # Simplified GARCH(1,1) using exponential weights
            alpha = 0.06
            beta = 0.92
            omega = np.var(returns) * (1 - alpha - beta)
            
            # Initialize
            sigma2 = np.var(returns)
            
            # Iterate through returns
            for ret in returns[-20:]:  # Use last 20 observations
                sigma2 = omega + alpha * ret**2 + beta * sigma2
                
            volatility = np.sqrt(sigma2 * 252)  # Annualized
            return float(volatility)
        except:
            return None
    
    def _calculate_ewma_volatility(self, prices: np.ndarray, lambda_param: float = 0.94) -> Optional[float]:
        """Calculate EWMA volatility"""
        try:
            if len(prices) < 10:
                return None
                
            returns = np.diff(np.log(prices))
            
            # EWMA volatility
            weights = np.array([(1 - lambda_param) * lambda_param**i for i in range(len(returns))])
            weights = weights[::-1] / weights.sum()
            
            ewma_var = np.sum(weights * returns**2)
            volatility = np.sqrt(ewma_var * 252)  # Annualized
            return float(volatility)
        except:
            return None
    
    def _calculate_volatility_ratio(self, prices: np.ndarray) -> Optional[float]:
        """Calculate short-term to long-term volatility ratio"""
        try:
            short_vol = self._calculate_realized_volatility(prices, 5)
            long_vol = self._calculate_realized_volatility(prices, 20)
            
            if short_vol and long_vol and long_vol > 0:
                return short_vol / long_vol
            return None
        except:
            return None
    
    def _detect_volatility_regime(self, prices: np.ndarray) -> Optional[str]:
        """Detect volatility regime (low, normal, high)"""
        try:
            vol = self._calculate_realized_volatility(prices, 20)
            if vol is None:
                return None
                
            # Simple regime classification
            if vol < 0.15:
                return "low"
            elif vol < 0.30:
                return "normal"
            else:
                return "high"
        except:
            return None
    
    def _calculate_iv_rank(self, options_data: Dict) -> Optional[float]:
        """Calculate IV rank from options data"""
        try:
            # Placeholder - would need historical IV data
            current_iv = options_data.get('implied_volatility')
            if current_iv:
                # Simplified rank calculation
                return min(100, max(0, current_iv * 100))
            return None
        except:
            return None
    
    def _calculate_iv_percentile(self, options_data: Dict) -> Optional[float]:
        """Calculate IV percentile"""
        try:
            # Placeholder - would need historical IV distribution
            return self._calculate_iv_rank(options_data)
        except:
            return None
    
    def _calculate_put_call_ratio(self, options_data: Dict) -> Optional[float]:
        """Calculate put/call ratio"""
        try:
            put_volume = options_data.get('put_volume', 0)
            call_volume = options_data.get('call_volume', 0)
            
            if call_volume > 0:
                return put_volume / call_volume
            return None
        except:
            return None
    
    def _calculate_options_skew(self, options_data: Dict) -> Optional[float]:
        """Calculate options volatility skew"""
        try:
            # Placeholder - would need full options chain
            return None
        except:
            return None
    
    def _calculate_var(self, prices: np.ndarray, confidence: float = 0.95, horizon: int = 1) -> Optional[float]:
        """Calculate Value at Risk"""
        try:
            if len(prices) < 30:
                return None
                
            returns = np.diff(np.log(prices))
            
            # Historical VaR
            var = np.percentile(returns, (1 - confidence) * 100)
            
            # Scale by horizon
            var_scaled = var * np.sqrt(horizon)
            
            return float(abs(var_scaled))
        except:
            return None
    
    def _calculate_max_drawdown(self, prices: np.ndarray) -> Optional[float]:
        """Calculate maximum drawdown"""
        try:
            if len(prices) < 2:
                return None
                
            # Calculate rolling maximum
            rolling_max = np.maximum.accumulate(prices)
            
            # Calculate drawdown
            drawdown = (prices - rolling_max) / rolling_max
            
            # Maximum drawdown
            max_dd = np.min(drawdown)
            
            return float(abs(max_dd))
        except:
            return None
    
    def _calculate_sharpe_ratio(self, prices: np.ndarray, risk_free_rate: float = 0.02) -> Optional[float]:
        """Calculate Sharpe ratio"""
        try:
            if len(prices) < 10:
                return None
                
            returns = np.diff(np.log(prices))
            
            # Annualized return and volatility
            annual_return = np.mean(returns) * 252
            annual_vol = np.std(returns) * np.sqrt(252)
            
            if annual_vol > 0:
                sharpe = (annual_return - risk_free_rate) / annual_vol
                return float(sharpe)
            return None
        except:
            return None
    
    def _calculate_sortino_ratio(self, prices: np.ndarray, risk_free_rate: float = 0.02) -> Optional[float]:
        """Calculate Sortino ratio"""
        try:
            if len(prices) < 10:
                return None
                
            returns = np.diff(np.log(prices))
            
            # Annualized return
            annual_return = np.mean(returns) * 252
            
            # Downside deviation
            negative_returns = returns[returns < 0]
            if len(negative_returns) > 0:
                downside_vol = np.std(negative_returns) * np.sqrt(252)
                sortino = (annual_return - risk_free_rate) / downside_vol
                return float(sortino)
            return None
        except:
            return None


class EnhancedMLEngine:
    """Enhanced ML engine with multiple algorithms and validation"""
    
    def __init__(self, config: EnhancedPipelineConfig):
        self.config = config
        self.models = {}
        self.feature_importance = {}
        self.scaler = StandardScaler()
        self._initialize_models()
        
    def _initialize_models(self):
        """Initialize ensemble models"""
        try:
            if 'rf' in self.config.ensemble_models:
                self.models['rf'] = RandomForestRegressor(
                    n_estimators=100,
                    max_depth=10,
                    random_state=42,
                    n_jobs=-1
                )
                
            if 'gbm' in self.config.ensemble_models:
                self.models['gbm'] = GradientBoostingRegressor(
                    n_estimators=100,
                    max_depth=6,
                    learning_rate=0.1,
                    random_state=42
                )
                
            logger.info(f"Initialized {len(self.models)} ML models")
        except Exception as e:
            logger.error(f"Failed to initialize ML models: {e}")
    
    def train_models(self, 
                    features_df: pd.DataFrame,
                    target: np.ndarray,
                    validation_split: float = 0.2) -> Dict[str, float]:
        """Train ensemble models with validation"""
        try:
            if features_df.empty or len(target) == 0:
                return {}
                
            # Prepare features
            X = features_df.fillna(0).values
            y = target
            
            # Scale features
            X_scaled = self.scaler.fit_transform(X)
            
            scores = {}
            
            # Train each model
            for name, model in self.models.items():
                try:
                    # Cross-validation
                    if self.config.model_complexity != ModelComplexity.SIMPLE:
                        cv_scores = cross_val_score(
                            model, X_scaled, y,
                            cv=TimeSeriesSplit(n_splits=self.config.cross_validation_folds),
                            scoring='neg_mean_squared_error'
                        )
                        scores[name] = float(-cv_scores.mean())
                    
                    # Fit model
                    model.fit(X_scaled, y)
                    
                    # Feature importance (if available)
                    if hasattr(model, 'feature_importances_'):
                        self.feature_importance[name] = model.feature_importances_
                        
                    logger.info(f"Trained {name} model")
                    
                except Exception as e:
                    logger.error(f"Failed to train {name} model: {e}")
                    continue
                    
            return scores
            
        except Exception as e:
            logger.error(f"Model training failed: {e}")
            return {}
    
    def predict_ensemble(self, features_df: pd.DataFrame) -> Dict[str, Any]:
        """Generate ensemble predictions"""
        try:
            if features_df.empty:
                return {'prediction': 0.0, 'confidence': 0.0}
                
            # Check if scaler is fitted
            if not hasattr(self.scaler, 'scale_'):
                # Auto-train with synthetic data if no training has occurred
                logger.info("No training data available, generating synthetic training data...")
                self._auto_train_with_synthetic_data(features_df)
                
            # Prepare features
            X = features_df.fillna(0).values
            X_scaled = self.scaler.transform(X)
            
            predictions = []
            weights = []
            
            # Get predictions from each model
            for name, model in self.models.items():
                try:
                    # Check if model is fitted
                    if not hasattr(model, 'feature_importances_') and not hasattr(model, 'estimators_'):
                        logger.debug(f"Model {name} not fitted, using fallback prediction")
                        # Fallback: simple feature-based prediction
                        pred = self._fallback_prediction(X_scaled[0])
                    else:
                        pred = model.predict(X_scaled)
                        pred = pred[0] if len(pred) > 0 else 0.0
                    
                    predictions.append(float(pred))
                    
                    # Weight by feature importance diversity (simplified)
                    weight = 1.0
                    if name in self.feature_importance:
                        weight = 1.0 + np.std(self.feature_importance[name])
                    weights.append(weight)
                    
                except Exception as e:
                    logger.debug(f"Prediction failed for {name}: {e}")
                    # Add fallback prediction
                    predictions.append(self._fallback_prediction(X_scaled[0] if len(X_scaled) > 0 else np.zeros(10)))
                    weights.append(1.0)
                    continue
            
            if not predictions:
                return {'prediction': 0.0, 'confidence': 0.0}
            
            # Weighted ensemble
            weights = np.array(weights)
            weights = weights / weights.sum() if weights.sum() > 0 else np.ones_like(weights) / len(weights)
            
            ensemble_pred = np.average(predictions, weights=weights)
            
            # Confidence from prediction variance (inverse relationship)
            pred_std = np.std(predictions) if len(predictions) > 1 else 0.0
            confidence = max(0.0, min(1.0, 1.0 - min(float(pred_std), 1.0)))
            
            return {
                'prediction': float(ensemble_pred),
                'confidence': float(confidence),
                'individual_predictions': predictions,
                'model_weights': weights.tolist()
            }
            
        except Exception as e:
            logger.error(f"Ensemble prediction failed: {e}")
            return {'prediction': 0.0, 'confidence': 0.5}  # Return neutral confidence
    
    def _auto_train_with_synthetic_data(self, features_df: pd.DataFrame):
        """Auto-train models with synthetic data when no training data available"""
        try:
            # Generate synthetic training data based on feature structure
            n_samples = 200
            n_features = len(features_df.columns)
            
            # Create synthetic features with realistic correlations
            X_synthetic = np.random.randn(n_samples, n_features)
            
            # Create synthetic target with some pattern
            # Simulate returns based on feature combinations
            y_synthetic = (
                X_synthetic[:, 0] * 0.1 +  # First feature has moderate impact
                X_synthetic[:, 1] * 0.05 + # Second feature has smaller impact
                np.random.normal(0, 0.02, n_samples)  # Add noise
            )
            
            # Train models
            synthetic_df = pd.DataFrame(X_synthetic, columns=features_df.columns)
            self.train_models(synthetic_df, y_synthetic)
            
            logger.info("Auto-training with synthetic data completed")
            
        except Exception as e:
            logger.error(f"Auto-training with synthetic data failed: {e}")
    
    def _fallback_prediction(self, features: np.ndarray) -> float:
        """Simple fallback prediction when models are not available"""
        try:
            if len(features) == 0:
                return 0.0
                
            # Simple heuristic based on feature values
            # Normalize features to [-1, 1] range
            features_norm = np.tanh(features)
            
            # Weighted sum with decreasing weights
            weights = np.exp(-np.arange(len(features_norm)) * 0.1)
            weights = weights / weights.sum()
            
            prediction = np.dot(features_norm, weights) * 0.1  # Scale to reasonable range
            
            return float(prediction)
            
        except Exception as e:
            logger.debug(f"Fallback prediction failed: {e}")
            return 0.0


# Safe mode initialization wrapper  
def safe_init_with_timeout(init_func, timeout_seconds: int = 5):
    """Initialize component with timeout protection"""
    
    def timeout_handler(signum, frame):
        raise TimeoutError("Initialization timeout")
    
    # Set up timeout signal
    old_handler = signal.signal(signal.SIGALRM, timeout_handler)
    signal.alarm(timeout_seconds)
    
    try:
        result = init_func()
        signal.alarm(0)  # Cancel timeout
        signal.signal(signal.SIGALRM, old_handler)  # Restore handler
        return result, None
    except TimeoutError:
        signal.alarm(0)
        signal.signal(signal.SIGALRM, old_handler)
        return None, "Initialization timeout"
    except Exception as e:
        signal.alarm(0)
        signal.signal(signal.SIGALRM, old_handler)
        return None, str(e)


class EnhancedOracleOptionsPipeline(BaseOptionsPipeline):
    """
    Enhanced Oracle Options Pipeline with advanced ML and robust error handling
    """
    
    def __init__(self, config: Optional[EnhancedPipelineConfig] = None):
        """Initialize the enhanced pipeline"""
        self.enhanced_config = config or EnhancedPipelineConfig()
        
        # Create base config from enhanced config for parent initialization
        base_config = PipelineConfig(
            risk_tolerance=self.enhanced_config.risk_tolerance,
            max_position_size=self.enhanced_config.max_position_size,
            min_opportunity_score=self.enhanced_config.min_opportunity_score,
            min_confidence=self.enhanced_config.min_confidence,
            preferred_strategies=self.enhanced_config.preferred_strategies,
            min_days_to_expiry=self.enhanced_config.min_days_to_expiry,
            max_days_to_expiry=self.enhanced_config.max_days_to_expiry,
            min_volume=self.enhanced_config.min_volume,
            min_open_interest=self.enhanced_config.min_open_interest,
            max_spread_ratio=self.enhanced_config.max_spread_ratio,
            max_workers=self.enhanced_config.max_workers,
            cache_ttl=self.enhanced_config.cache_ttl,
            safe_mode=self.enhanced_config.safe_mode == SafeMode.MINIMAL
        )
        
        # Initialize base pipeline
        super().__init__(base_config)
        
        logger.info(f"Initializing Enhanced Oracle Options Pipeline in {self.enhanced_config.safe_mode.value} mode...")
        
        # Enhanced components
        self.feature_engine = EnhancedFeatureEngine(self.enhanced_config)
        self.ml_engine = EnhancedMLEngine(self.enhanced_config)
        
        # Safe component initialization
        self._initialize_components()
        
        # Thread pool for parallel processing
        self.executor = ThreadPoolExecutor(max_workers=self.config.max_workers)
        
        # Cache for results
        self._cache = {}
        self._cache_timestamps = {}
        
        # Performance monitoring
        self.performance_stats = {
            'predictions_made': 0,
            'accuracy_scores': [],
            'execution_times': [],
            'error_count': 0
        }
        
        logger.info("Enhanced Oracle Options Pipeline initialized successfully")
    
    def _initialize_components(self):
        """Initialize components with timeout and fallback handling"""
        
        # Data orchestrator initialization (problematic component)
        if self.config.safe_mode == SafeMode.FULL:
            try:
                from data_feeds.data_feed_orchestrator import DataFeedOrchestrator
                
                def init_orchestrator():
                    return DataFeedOrchestrator()
                
                orchestrator, error = safe_init_with_timeout(
                    init_orchestrator,
                    self.enhanced_config.orchestrator_init_timeout
                )
                
                if error:
                    logger.warning(f"DataFeedOrchestrator initialization failed: {error}")
                    logger.info("Switching to safe mode...")
                    self.enhanced_config.safe_mode = SafeMode.SAFE
                else:
                    self.orchestrator = orchestrator
                    logger.info("DataFeedOrchestrator initialized successfully")
                    
            except Exception as e:
                logger.warning(f"DataFeedOrchestrator import failed: {e}")
                self.enhanced_config.safe_mode = SafeMode.SAFE
        
        # Safe mode fallbacks
        if self.enhanced_config.safe_mode in [SafeMode.SAFE, SafeMode.MINIMAL]:
            logger.info("Using safe mode with mock data generation")
            self.orchestrator = None  # Use mock data
            
        # Valuation engine initialization
        try:
            if self.enhanced_config.safe_mode != SafeMode.MINIMAL:
                from data_feeds.options_valuation_engine import OptionsValuationEngine
                self.valuation_engine = OptionsValuationEngine(
                    cache_ttl=self.enhanced_config.cache_ttl,
                    confidence_threshold=self.enhanced_config.min_confidence,
                    max_workers=self.enhanced_config.max_workers
                )
                logger.info("Options valuation engine initialized")
        except Exception as e:
            logger.warning(f"Valuation engine initialization failed: {e}")
            self.valuation_engine = None
    
    def analyze_symbol_enhanced(self,
                               symbol: str,
                               target_date: Optional[str] = None) -> List[Dict[str, Any]]:
        """Enhanced symbol analysis with advanced features"""
        
        logger.info(f"Enhanced analysis for {symbol}...")
        
        try:
            # Get market data (with fallback to mock)
            market_data = self._get_market_data_safe(symbol)
            
            # Extract advanced features
            features = self.feature_engine.extract_features(symbol, market_data)
            
            # Generate ML predictions
            feature_dict = asdict(features)
            
            # Convert categorical features to numeric for ML
            if 'volatility_regime' in feature_dict:
                volatility_map = {'low': 0, 'normal': 1, 'high': 2}
                feature_dict['volatility_regime'] = volatility_map.get(feature_dict['volatility_regime'], 1)
            
            features_df = pd.DataFrame([feature_dict])
            
            # Remove None values and convert to appropriate types
            for col in features_df.columns:
                features_df[col] = features_df[col].fillna(0)
                # Ensure numeric types
                if features_df[col].dtype == 'object':
                    try:
                        features_df[col] = pd.to_numeric(features_df[col], errors='coerce').fillna(0)
                    except:
                        features_df[col] = 0
            
            # Get ML predictions
            ml_result = self.ml_engine.predict_ensemble(features_df)
            
            # Generate comprehensive recommendations
            recommendations = self._generate_enhanced_recommendations(
                symbol,
                features,
                ml_result,
                market_data
            )
            
            # Update performance stats
            self.performance_stats['predictions_made'] += 1
            
            return recommendations
            
        except Exception as e:
            logger.error(f"Enhanced analysis failed for {symbol}: {e}")
            self.performance_stats['error_count'] += 1
            return []
    
    def _get_market_data_safe(self, symbol: str, period: str = "1mo") -> pd.DataFrame:
        """Get market data with safe fallbacks"""
        
        if self.orchestrator:
            try:
                market_data = self.orchestrator.get_market_data(symbol, period=period)
                if market_data and not market_data.data.empty:
                    return market_data.data
            except Exception as e:
                logger.debug(f"Real data fetch failed for {symbol}: {e}")
        
        # Generate mock market data
        return self._generate_mock_market_data(symbol, period)
    
    def _generate_mock_market_data(self, symbol: str, period: str = "1mo") -> pd.DataFrame:
        """Generate realistic mock market data for testing"""
        
        # Determine number of days
        days_map = {"1d": 1, "5d": 5, "1mo": 30, "3mo": 90, "1y": 252}
        days = days_map.get(period, 30)
        
        # Base prices for different symbols
        base_prices = {
            'AAPL': 175.0, 'NVDA': 420.0, 'GOOGL': 162.0, 'MSFT': 350.0,
            'SPY': 543.0, 'QQQ': 400.0, 'TSLA': 250.0, 'META': 300.0
        }
        
        base_price = base_prices.get(symbol, 100.0)
        
        # Generate price series with realistic patterns
        np.random.seed(hash(symbol) % 2**31)  # Consistent seed per symbol
        
        # Parameters
        drift = 0.0002  # Daily drift
        volatility = 0.02  # Daily volatility
        
        dates = pd.date_range(end=datetime.now(), periods=days, freq='D')
        
        # Generate returns
        returns = np.random.normal(drift, volatility, days)
        
        # Add some autocorrelation and trends
        for i in range(1, len(returns)):
            returns[i] += 0.1 * returns[i-1]  # Momentum
        
        # Generate price series
        prices = [base_price]
        for ret in returns[1:]:
            prices.append(prices[-1] * (1 + ret))
        
        # Create OHLCV data
        data = []
        for i, (date, close) in enumerate(zip(dates, prices)):
            # Generate OHLC from close price
            volatility_factor = np.random.uniform(0.005, 0.02)
            high = close * (1 + volatility_factor)
            low = close * (1 - volatility_factor)
            open_price = prices[i-1] if i > 0 else close
            volume = np.random.randint(1000000, 5000000)
            
            data.append({
                'date': date,
                'open': open_price,
                'high': high,
                'low': low,
                'close': close,
                'volume': volume
            })
        
        df = pd.DataFrame(data)
        df.set_index('date', inplace=True)
        
        return df
    
    def _generate_enhanced_recommendations(self,
                                          symbol: str,
                                          features: AdvancedFeatures,
                                          ml_result: Dict[str, Any],
                                          market_data: pd.DataFrame) -> List[Dict[str, Any]]:
        """Generate enhanced recommendations using all available features"""
        
        recommendations = []
        
        try:
            current_price = market_data['close'].iloc[-1] if not market_data.empty else 100.0
            
            # Calculate opportunity score using multiple factors
            opportunity_score = self._calculate_enhanced_opportunity_score(features, ml_result)
            
            if opportunity_score < self.config.min_opportunity_score:
                return []
            
            # Generate call and put recommendations
            strategies = [
                ('CALL', current_price * 1.05),  # 5% OTM call
                ('PUT', current_price * 0.95),   # 5% OTM put
            ]
            
            for option_type, strike in strategies:
                
                # Calculate expiry date (30 days out)
                expiry_date = datetime.now() + timedelta(days=30)
                
                # Estimate option price using simple Black-Scholes approximation
                time_to_expiry = 30 / 365
                volatility = features.realized_volatility_20d or 0.25
                risk_free_rate = 0.04
                
                option_price = self._estimate_option_price(
                    current_price, strike, time_to_expiry, volatility, risk_free_rate, option_type
                )
                
                # Calculate trade parameters
                entry_price = option_price
                
                # Target based on ML prediction
                ml_prediction = ml_result.get('prediction', 0.0)
                if option_type == 'CALL':
                    target_price = entry_price * (1 + abs(ml_prediction))
                else:
                    target_price = entry_price * (1 + abs(ml_prediction * 0.8))  # Puts typically less leverage
                
                stop_loss = entry_price * 0.7  # 30% stop loss
                
                # Position sizing with Kelly Criterion
                position_size = self._calculate_kelly_position_size(
                    ml_result.get('confidence', 0.5),
                    abs(ml_prediction),
                    opportunity_score
                )
                
                # Risk metrics
                max_loss = entry_price * position_size
                expected_return = (target_price - entry_price) / entry_price
                risk_reward = (target_price - entry_price) / (entry_price - stop_loss) if stop_loss < entry_price else 0
                
                # Breakeven calculation
                if option_type == 'CALL':
                    breakeven = strike + entry_price
                else:
                    breakeven = strike - entry_price
                
                # Generate key reasons based on features
                key_reasons = self._generate_key_reasons(features, ml_result, option_type)
                
                # Risk factors
                risk_factors = self._generate_risk_factors(features, option_type)
                
                recommendation = {
                    'symbol': symbol,
                    'strategy': f'long_{option_type.lower()}',
                    'contract': {
                        'type': option_type,
                        'strike': round(strike, 2),
                        'expiry': expiry_date.strftime('%Y-%m-%d'),
                        'bid': round(option_price * 0.98, 2),  # Bid slightly below estimate
                        'ask': round(option_price * 1.02, 2),  # Ask slightly above estimate
                        'last': round(option_price, 2),
                        'volume': np.random.randint(100, 2000),
                        'open_interest': np.random.randint(500, 5000),
                        'underlying_price': round(current_price, 2)
                    },
                    'scores': {
                        'opportunity': round(opportunity_score, 1),
                        'ml_confidence': round(ml_result.get('confidence', 0.5), 3),
                        'valuation': round(abs(ml_prediction), 3)
                    },
                    'trade': {
                        'entry_price': round(entry_price, 2),
                        'target_price': round(target_price, 2),
                        'stop_loss': round(stop_loss, 2),
                        'position_size': round(position_size, 3),
                        'max_contracts': max(1, int(position_size * 10000 / (entry_price * 100)))
                    },
                    'risk': {
                        'max_loss': round(max_loss, 2),
                        'expected_return': round(expected_return, 3),
                        'probability_of_profit': round(ml_result.get('confidence', 0.5), 3),
                        'risk_reward_ratio': round(risk_reward, 2),
                        'breakeven_price': round(breakeven, 2)
                    },
                    'analysis': {
                        'key_reasons': key_reasons,
                        'risk_factors': risk_factors,
                        'entry_signals': self._generate_entry_signals(features)
                    },
                    'advanced_features': {
                        'rsi': features.rsi,
                        'macd': features.macd,
                        'volatility_regime': features.volatility_regime,
                        'iv_rank': features.iv_rank,
                        'var_1d': features.var_1d,
                        'sharpe_ratio': features.sharpe_ratio
                    },
                    'metadata': {
                        'timestamp': datetime.now().isoformat(),
                        'data_quality': 0.85,  # Mock quality score
                        'model_confidence': ml_result.get('confidence', 0.5),
                        'feature_count': len([v for v in asdict(features).values() if v is not None])
                    }
                }
                
                recommendations.append(recommendation)
        
        except Exception as e:
            logger.error(f"Failed to generate enhanced recommendations for {symbol}: {e}")
        
        # Sort by opportunity score
        recommendations.sort(key=lambda x: x['scores']['opportunity'], reverse=True)
        
        return recommendations
    
    def _calculate_enhanced_opportunity_score(self,
                                             features: AdvancedFeatures,
                                             ml_result: Dict[str, Any]) -> float:
        """Calculate enhanced opportunity score using multiple factors"""
        
        score_components = []
        
        # ML prediction component (40%)
        ml_confidence = ml_result.get('confidence', 0.5)
        ml_prediction = abs(ml_result.get('prediction', 0.0))
        ml_score = (ml_confidence * 0.6 + min(1.0, ml_prediction * 2) * 0.4) * 100
        score_components.append(('ml', ml_score, 0.4))
        
        # Technical indicators component (30%)
        tech_score = 50  # Neutral base
        
        # RSI contribution
        if features.rsi is not None:
            if features.rsi < 30:  # Oversold
                tech_score += 15
            elif features.rsi > 70:  # Overbought (good for puts)
                tech_score += 10
        
        # MACD contribution
        if features.macd is not None and features.macd_signal is not None:
            if features.macd > features.macd_signal:  # Bullish crossover
                tech_score += 10
                
        # Bollinger bands contribution
        if features.bollinger_position is not None:
            if features.bollinger_position < 0.2:  # Near lower band
                tech_score += 10
            elif features.bollinger_position > 0.8:  # Near upper band
                tech_score += 5
        
        score_components.append(('technical', min(100, tech_score), 0.3))
        
        # Volatility component (20%)
        vol_score = 50  # Neutral base
        
        if features.volatility_regime:
            if features.volatility_regime == "low":
                vol_score += 20  # Good for long options
            elif features.volatility_regime == "normal":
                vol_score += 10
        
        if features.volatility_ratio is not None:
            if features.volatility_ratio > 1.2:  # Rising volatility
                vol_score += 15
        
        score_components.append(('volatility', min(100, vol_score), 0.2))
        
        # Risk-adjusted component (10%)
        risk_score = 50  # Neutral base
        
        if features.sharpe_ratio is not None:
            if features.sharpe_ratio > 1.0:
                risk_score += 25
            elif features.sharpe_ratio > 0.5:
                risk_score += 15
        
        if features.max_drawdown is not None:
            if features.max_drawdown < 0.1:  # Low drawdown
                risk_score += 15
        
        score_components.append(('risk', min(100, risk_score), 0.1))
        
        # Calculate weighted score
        total_score = sum(score * weight for _, score, weight in score_components)
        
        return min(100, max(0, total_score))
    
    def _estimate_option_price(self,
                              S: float,  # Current stock price
                              K: float,  # Strike price
                              T: float,  # Time to expiry
                              sigma: float,  # Volatility
                              r: float,  # Risk-free rate
                              option_type: str) -> float:
        """Estimate option price using Black-Scholes"""
        
        try:
            from scipy.stats import norm
            
            # Black-Scholes calculation
            d1 = (np.log(S / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))
            d2 = d1 - sigma * np.sqrt(T)
            
            if option_type == 'CALL':
                price = S * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)
            else:  # PUT
                price = K * np.exp(-r * T) * norm.cdf(-d2) - S * norm.cdf(-d1)
            
            return max(0.01, price)  # Minimum option price
            
        except Exception as e:
            logger.debug(f"Black-Scholes calculation failed: {e}")
            # Fallback to intrinsic value plus time value
            if option_type == 'CALL':
                intrinsic = max(0, S - K)
            else:
                intrinsic = max(0, K - S)
            
            time_value = 0.05 * S * np.sqrt(T) * sigma  # Simplified time value
            return intrinsic + time_value
    
    def _calculate_kelly_position_size(self,
                                      confidence: float,
                                      expected_return: float,
                                      opportunity_score: float) -> float:
        """Calculate position size using Kelly Criterion"""
        
        # Kelly formula: f = (bp - q) / b
        # where b = odds, p = probability of win, q = probability of loss
        
        # Estimate win probability from confidence and opportunity score
        win_prob = (confidence + opportunity_score / 100) / 2
        loss_prob = 1 - win_prob
        
        # Estimate odds from expected return
        odds = max(1.0, abs(expected_return) * 2)  # Conservative odds
        
        # Kelly fraction
        kelly_fraction = (odds * win_prob - loss_prob) / odds
        
        # Apply conservative scaling
        kelly_fraction = max(0, min(0.25, kelly_fraction * 0.5))  # Max 25%, scale by 50%
        
        # Adjust by risk tolerance
        if self.config.risk_tolerance == RiskTolerance.CONSERVATIVE:
            kelly_fraction *= 0.5
        elif self.config.risk_tolerance == RiskTolerance.AGGRESSIVE:
            kelly_fraction *= 1.5
        
        return min(kelly_fraction, self.config.max_position_size)
    
    def _generate_key_reasons(self,
                             features: AdvancedFeatures,
                             ml_result: Dict[str, Any],
                             option_type: str) -> List[str]:
        """Generate key reasons for the recommendation"""
        
        reasons = []
        
        # ML-based reasons
        ml_confidence = ml_result.get('confidence', 0.5)
        if ml_confidence > 0.7:
            reasons.append(f"High ML confidence: {ml_confidence:.1%}")
        
        # Technical reasons
        if features.rsi is not None:
            if option_type == 'CALL' and features.rsi < 35:
                reasons.append(f"RSI oversold: {features.rsi:.1f}")
            elif option_type == 'PUT' and features.rsi > 65:
                reasons.append(f"RSI overbought: {features.rsi:.1f}")
        
        # Volatility reasons
        if features.volatility_regime == "low":
            reasons.append("Low volatility environment favorable for long options")
        
        if features.volatility_ratio is not None and features.volatility_ratio > 1.2:
            reasons.append("Rising short-term volatility")
        
        # Options-specific reasons
        if features.iv_rank is not None and features.iv_rank < 30:
            reasons.append(f"Low IV rank: {features.iv_rank:.0f}")
        
        # Risk-adjusted reasons
        if features.sharpe_ratio is not None and features.sharpe_ratio > 1.0:
            reasons.append(f"Strong risk-adjusted returns: Sharpe {features.sharpe_ratio:.2f}")
        
        return reasons[:5]  # Limit to top 5 reasons
    
    def _generate_risk_factors(self,
                              features: AdvancedFeatures,
                              option_type: str) -> List[str]:
        """Generate risk factors for the recommendation"""
        
        risks = []
        
        # Volatility risks
        if features.volatility_regime == "high":
            risks.append("High volatility environment increases risk")
        
        # Time decay risks
        risks.append("Time decay accelerates closer to expiration")
        
        # Technical risks
        if features.rsi is not None:
            if option_type == 'CALL' and features.rsi > 75:
                risks.append("Overbought conditions may limit upside")
            elif option_type == 'PUT' and features.rsi < 25:
                risks.append("Oversold conditions may limit downside")
        
        # Market risks
        if features.max_drawdown is not None and features.max_drawdown > 0.2:
            risks.append("High historical drawdown indicates volatility")
        
        # Options-specific risks
        risks.append("Options can expire worthless")
        
        return risks[:4]  # Limit to top 4 risks
    
    def _generate_entry_signals(self, features: AdvancedFeatures) -> List[str]:
        """Generate entry signals based on features"""
        
        signals = []
        
        # MACD signals
        if (features.macd is not None and features.macd_signal is not None and
            features.macd_histogram is not None):
            if features.macd > features.macd_signal and features.macd_histogram > 0:
                signals.append("MACD bullish crossover")
        
        # Bollinger band signals
        if features.bollinger_position is not None:
            if features.bollinger_position < 0.25:
                signals.append("Price near lower Bollinger band")
            elif features.bollinger_position > 0.75:
                signals.append("Price near upper Bollinger band")
        
        # Volatility signals
        if features.volatility_ratio is not None and features.volatility_ratio > 1.3:
            signals.append("Volatility expansion signal")
        
        return signals[:3]  # Limit to top 3 signals
    
    def generate_market_scan(self,
                            symbols: Optional[List[str]] = None,
                            max_symbols: int = 20) -> Dict[str, Any]:
        """Generate enhanced market scan with advanced analytics"""
        
        if not symbols:
            symbols = ['SPY', 'QQQ', 'AAPL', 'NVDA', 'GOOGL', 'MSFT', 'TSLA', 'META'][:max_symbols]
        
        logger.info(f"Enhanced market scan for {len(symbols)} symbols...")
        
        start_time = time.time()
        all_recommendations = []
        errors = []
        
        # Process symbols with enhanced analysis
        with ThreadPoolExecutor(max_workers=self.config.max_workers) as executor:
            futures = {
                executor.submit(self.analyze_symbol_enhanced, symbol): symbol
                for symbol in symbols
            }
            
            for future in as_completed(futures):
                symbol = futures[future]
                try:
                    recommendations = future.result()
                    all_recommendations.extend(recommendations)
                except Exception as e:
                    error_msg = f"Failed to analyze {symbol}: {e}"
                    logger.error(error_msg)
                    errors.append(error_msg)
        
        # Sort by opportunity score
        all_recommendations.sort(key=lambda x: x['scores']['opportunity'], reverse=True)
        
        execution_time = time.time() - start_time
        
        # Generate summary statistics
        summary = {
            'scan_results': {
                'symbols_analyzed': len(symbols),
                'opportunities_found': len(all_recommendations),
                'execution_time': round(execution_time, 2),
                'errors': len(errors)
            },
            'top_opportunities': all_recommendations[:10],
            'performance_stats': self.performance_stats,
            'market_insights': self._generate_market_insights(all_recommendations),
            'timestamp': datetime.now().isoformat()
        }
        
        logger.info(f"Enhanced market scan complete: {len(all_recommendations)} opportunities in {execution_time:.2f}s")
        
        return summary
    
    def _generate_market_insights(self, recommendations: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Generate market insights from recommendations"""
        
        if not recommendations:
            return {}
        
        insights = {}
        
        # Opportunity distribution
        scores = [rec['scores']['opportunity'] for rec in recommendations]
        insights['opportunity_distribution'] = {
            'mean': round(np.mean(scores), 1),
            'median': round(np.median(scores), 1),
            'std': round(np.std(scores), 1),
            'min': round(min(scores), 1),
            'max': round(max(scores), 1)
        }
        
        # Strategy distribution
        strategies = [rec['strategy'] for rec in recommendations]
        strategy_counts = {}
        for strategy in strategies:
            strategy_counts[strategy] = strategy_counts.get(strategy, 0) + 1
        insights['strategy_distribution'] = strategy_counts
        
        # Volatility regime analysis
        vol_regimes = []
        for rec in recommendations:
            if 'advanced_features' in rec and rec['advanced_features'].get('volatility_regime'):
                vol_regimes.append(rec['advanced_features']['volatility_regime'])
        
        if vol_regimes:
            regime_counts = {}
            for regime in vol_regimes:
                regime_counts[regime] = regime_counts.get(regime, 0) + 1
            insights['volatility_regimes'] = regime_counts
        
        # Average confidence
        confidences = [rec['scores']['ml_confidence'] for rec in recommendations]
        insights['average_ml_confidence'] = round(np.mean(confidences), 3)
        
        return insights
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """Get comprehensive performance summary"""
        
        return {
            'performance_stats': self.performance_stats,
            'pipeline_config': {
                'safe_mode': self.config.safe_mode.value,
                'model_complexity': self.config.model_complexity.value,
                'max_workers': self.config.max_workers,
                'enabled_features': {
                    'advanced_features': self.config.enable_advanced_features,
                    'var_calculation': self.config.enable_var_calculation,
                    'stress_testing': self.config.enable_stress_testing,
                    'backtesting': self.config.enable_backtesting
                }
            },
            'cache_stats': {
                'cache_size': len(self._cache),
                'cache_hit_rate': 0.0  # Would track in production
            },
            'ml_engine_stats': {
                'models_available': len(self.ml_engine.models),
                'feature_importance_tracked': len(self.ml_engine.feature_importance)
            },
            'timestamp': datetime.now().isoformat()
        }
    
    def shutdown(self):
        """Clean shutdown of enhanced pipeline"""
        logger.info("Shutting down Enhanced Oracle Options Pipeline...")
        
        # Clear caches
        self._cache.clear()
        self._cache_timestamps.clear()
        
        # Shutdown executor
        self.executor.shutdown(wait=True)
        
        logger.info("Enhanced pipeline shutdown complete")


# Factory function for easy initialization
def create_enhanced_pipeline(config: Optional[Dict[str, Any]] = None) -> EnhancedOracleOptionsPipeline:
    """
    Create and initialize an Enhanced Oracle Options Pipeline
    
    Args:
        config: Configuration dictionary
        
    Returns:
        Initialized enhanced pipeline
    """
    if config:
        enhanced_config = EnhancedPipelineConfig(**config)
    else:
        enhanced_config = EnhancedPipelineConfig()
    
    return EnhancedOracleOptionsPipeline(enhanced_config)


if __name__ == "__main__":
    # Example usage
    pipeline = create_enhanced_pipeline({
        'safe_mode': SafeMode.SAFE,
        'model_complexity': ModelComplexity.MODERATE,
        'enable_advanced_features': True
    })
    
    # Run market scan
    results = pipeline.generate_market_scan(['AAPL', 'NVDA', 'SPY'])
    
    print("Enhanced Oracle-X Options Pipeline Demo")
    print("=" * 50)
    print(f"Opportunities found: {results['scan_results']['opportunities_found']}")
    print(f"Execution time: {results['scan_results']['execution_time']}s")
    
    if results['top_opportunities']:
        print("\nTop Opportunity:")
        top = results['top_opportunities'][0]
        print(f"Symbol: {top['symbol']}")
        print(f"Strategy: {top['strategy']}")
        print(f"Opportunity Score: {top['scores']['opportunity']}")
        print(f"ML Confidence: {top['scores']['ml_confidence']:.1%}")
    
    pipeline.shutdown()
